{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b82e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu118'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\":\"black\",\n",
    "    \"axes.facecolor\":\"black\",\n",
    "    \"axes.edgecolor\":\"orange\",\n",
    "    \"xtick.color\":\"red\",\n",
    "    \"ytick.color\":\"yellow\",\n",
    "    \"legend.labelcolor\":\"linecolor\"\n",
    "})\n",
    "plt.plot(range(10))\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll use linear regression to create the data with known parameters(things that can be learned by a model) and then we'll\n",
    "# use PyTorch to see if we can build model to estimate these parameters using gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef06c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d6ac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ea4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create known parameters\n",
    "weight =.7\n",
    "bias =.3\n",
    "import torch\n",
    "# Create data\n",
    "start = 0\n",
    "end = 1\n",
    "step = .02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight* X + bias\n",
    "X[:10], y[:10]\n",
    "# Now we are moving forward towards building a model that can learn the relationship between X (features) and y(labels)\n",
    "# Creating manual train test split\n",
    "train_split = int(.8* len(X)) #80% of data used for training set, 20% for testing\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)\n",
    "import matplotlib.pyplot as plt\n",
    "# Till our data is just nos. on page now creating a function to visualize it\n",
    "def plot_predictions(train_data=X_train, \n",
    "                   train_labels=y_train,\n",
    "                    test_data=X_test,\n",
    "                    test_labels=y_test,\n",
    "                    predictions=None\n",
    "                   ):\n",
    "    #     Plots training data, test data and compares predictions \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training Data\")\n",
    "    #Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing Data\")\n",
    "    if predictions is not None:\n",
    "        #Plot the predictions in Red (predictions were made on the test datat)\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "    #Show the legend\n",
    "    plt.legend(prop={\"size\":14})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ec51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Linear RegressionModel(nn.Module):\n",
    "import torch\n",
    "from torch import nn\n",
    "class LinearRegressionModel(nn.Module): #<---almost everythings in PyTorch is nn.Module(neural network type)\n",
    "    # arg--->nn.Module contains all the building blocks for neural networks\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, dtype=torch.float), requires_grad=True)\n",
    "        # PyTorch lobes float32 by default \n",
    "        # True represents that value can be updated via gradient descent method\n",
    "        self.bias = nn.Parameter(torch.randn(1, dtype=torch.float), requires_grad=True)\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # any subclass of nn.Module needs to override forward() method\n",
    "#         x is the input data\n",
    "        return self.weights*x + self.bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722d8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PyTorch Modules\n",
    "# # torch.nn: contains all the building blocks for computational graphs\n",
    "# torch.nn.Parameter: stores tensors that can be used with nn.Module. if requires_grad=True gradients(used for updating model \n",
    "#  parameter via Gradient Descent) are calculated automatically.\n",
    "# torch.nn.Module:base class for all neural network modules, all the building blocks for neural networks are subclasses. if you\n",
    "#  are building a neural network in PyTorch, your models should subclass nn.Module\n",
    "# torch.optim: contains various optimization algorithm\n",
    "# def forward() all nn.module subclass requires a forward() method defines the future required computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ebfe332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Checking the contents of the PyTorch model\n",
    "# Let's create a model instance with the class we've made and check it's parameters usings\n",
    "# .parameters()\n",
    "# Manually setting seed since nn.Parameter are randomly initialized\n",
    "torch.manual_seed(42)\n",
    "# Creating an instance of the class\n",
    "model_0 = LinearRegressionModel()\n",
    "# Checkiing the nn.Parameter within the nn.Module subclass we created\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270a25d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Listing the named parameters\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be852a5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Making predictions using torch.inference_mode()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m----> 3\u001b[0m     y_preds \u001b[38;5;241m=\u001b[39m model_0(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Making predictions using torch.inference_mode()\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4afb73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.inference_mode() & torch.no_grad() do similar things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977eb68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Chcking the predictions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of testing samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of predictions made: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y_preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted value: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m+\u001b[39mpreds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Chcking the predictions\n",
    "print(f\"Number of testing samples: {len(X_test)}\")\n",
    "print(f\"Number of predictions made: {len(y_preds)}\")\n",
    "print(f\"Predicted value: \\n{y+preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function\n",
    "loss_fn =nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
